<?xml version="1.0" encoding="UTF-8"?>
<section id="kubectl.yaml">
	<title>Kubernetes YAML</title>
	<section id="ServiceAccount">
		<title>ServiceAccount</title>
		<screen>
		<![CDATA[
apiVersion: v1
kind: ServiceAccount
metadata:
 labels:
   app: elasticsearch
 name: elasticsearch
 namespace: elastic		
		]]>
		</screen>
	</section>

	<section id="Namespace">
		<title>创建命名空间</title>
		<para> 创建 jenkins-namespace.yaml</para>
		<screen>
			<![CDATA[
apiVersion: v1
kind: Namespace
metadata:
  name: jenkins-project
			]]>
		</screen>
		<screen>
			<![CDATA[
$ kubectl create -f jenkins-namespace.yaml
namespace ”jenkins-project“ created			
			]]>
		</screen>
	</section>
	<section id="Pod">
		<title>Pod</title>
		<screen>
		<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done']		
		]]>
		</screen>
		<para>创建 pod</para>
		<screen>
		<![CDATA[
iMac:kubernetes neo$ kubectl create -f pod.yaml 
pod/counter created

iMac:kubernetes neo$ kubectl logs counter
0: Sun Oct  4 12:32:44 UTC 2020
1: Sun Oct  4 12:32:45 UTC 2020
2: Sun Oct  4 12:32:46 UTC 2020
3: Sun Oct  4 12:32:47 UTC 2020
4: Sun Oct  4 12:32:48 UTC 2020
5: Sun Oct  4 12:32:49 UTC 2020
6: Sun Oct  4 12:32:50 UTC 2020
7: Sun Oct  4 12:32:51 UTC 2020
8: Sun Oct  4 12:32:52 UTC 2020
9: Sun Oct  4 12:32:53 UTC 2020
		]]>
		</screen>
		<section id="pod.spec.containers.imagePullPolicy">
			<title>容器编排</title>
			<section id="imagePullPolicy ">
				<title>镜像拉取策略</title>
				<para>imagePullPolicy: Always 总是拉取</para>
				<para>imagePullPolicy: IfNotPresent 默认值,本地有则使用本地镜像,不拉取</para>
				<para>imagePullPolicy: Never 只使用本地镜像，从不拉取</para>
			</section>
		</section>
		<section id="pod.spec.hostAliases">
			<title>指定主机名</title>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: hostaliases-pod
spec:
  restartPolicy: Never
  hostAliases:
  - ip: "127.0.0.1"
    hostnames:
    - "foo.local"
    - "bar.local"
  - ip: "10.1.2.3"
    hostnames:
    - "foo.remote"
    - "bar.remote"
  containers:
  - name: cat-hosts
    image: busybox
    command:
    - cat
    args:
    - "/etc/hosts"			
			]]>
			</screen>
		</section>
		<section id="pod.spec.containers.env">
			<title>环境变量</title>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: envars-fieldref
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/busybox
      command: [ "sh", "-c"]
      args:
      - while true; do
          echo -en '\n';
          printenv NODE_NAME POD_NAME POD_NAMESPACE;
          printenv POD_IP POD_SERVICE_ACCOUNT;
          sleep 10;
        done;
      env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
  restartPolicy: Never			
			]]>
			</screen>
			<para></para>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: envars-resourcefieldref
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/busybox:1.24
      command: [ "sh", "-c"]
      args:
      - while true; do
          echo -en '\n';
          printenv CPU_REQUEST CPU_LIMIT;
          printenv MEM_REQUEST MEM_LIMIT;
          sleep 10;
        done;
      resources:
        requests:
          memory: "32Mi"
          cpu: "125m"
        limits:
          memory: "64Mi"
          cpu: "250m"
      env:
        - name: CPU_REQUEST
          valueFrom:
            resourceFieldRef:
              containerName: test-container
              resource: requests.cpu
        - name: CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: test-container
              resource: limits.cpu
        - name: MEM_REQUEST
          valueFrom:
            resourceFieldRef:
              containerName: test-container
              resource: requests.memory
        - name: MEM_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: test-container
              resource: limits.memory
  restartPolicy: Never			
			]]>
			</screen>
		</section>
		<section id="pod.spec.containers.readinessProbe">
			<title>健康状态检查</title>
			<para>就绪探针</para>
			<screen>
			<![CDATA[
        readinessProbe: 
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 10         #10s之后开始第一次探测
          periodSeconds: 5                #第一次探测之后每隔5s探测一次			
			]]>
			</screen>
			<para></para>
		</section>
		<section id="pod.spec.securityContext">
			<title>securityContext</title>
			<section>
				<title>sysctls</title>
				<para></para>
				<screen>
				<![CDATA[
kubelet --allowed-unsafe-sysctls \
  'kernel.msg*,net.core.somaxconn' ...				
				]]>
				</screen>
				<screen>
				<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example
spec:
  securityContext:
    sysctls:
    - name: kernel.shm_rmid_forced
      value: "0"
    - name: net.core.somaxconn
      value: "1024"
    - name: kernel.msgmax
      value: "65536"				
				]]>
				</screen>
			</section>
			<section>
				<title>runAsUser</title>
				<para>allowPrivilegeEscalation 表示是否继承父进程权限，runAsUser 表示使用 UID 1000 的用户运行</para>
				<screen>
				<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 1000
  containers:
  - name: sec-ctx-demo
    image: busybox:latest
    securityContext:
      runAsUser: 1000
      allowPrivilegeEscalation: false				
				]]>
				</screen>
				<screen>
				<![CDATA[
   spec:
     securityContext:
        runAsUser: 1000
        fsGroup: 2000
        runAsNonRoot: true				
				]]>
				</screen>
			</section>

			<section>
				<title>security.alpha.kubernetes.io/sysctls</title>
				<para>security.alpha.kubernetes.io/sysctls</para>
				<screen>
			<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example
  annotations:
    security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=1
spec:			
			]]>
				</screen>
				<para>unsafe-sysctls</para>
				<screen>
			<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example
  annotations:
    security.alpha.kubernetes.io/unsafe-sysctls: net.core.somaxconn=65535                 #使用unsafe sysctl，设置最大连接数
spec:
  securityContext:
    privileged: true                                                                      #开启privileged权限			
			]]>
				</screen>
			</section>
		</section>
		<section>
			<title>nodeName 选择节点</title>
			<para>首先查看节点名称</para>
			<screen>
			<![CDATA[
[root@master ~]# kubectl get node
NAME      STATUS   ROLES                  AGE     VERSION
agent-1   Ready    <none>                 2d13h   v1.24.4+k3s1
master    Ready    control-plane,master   2d13h   v1.24.4+k3s1
agent-2   Ready    <none>                 13h     v1.24.4+k3s1			
			]]>
			</screen>
			<para>使用 nodeName: master 选择节点</para>
			<screen>
			<![CDATA[
metadata:
  name: redis
  labels:
    app: redis
spec:
  replicas: 1
  serviceName: redis
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:latest
          ports:
            - containerPort: 6379
          volumeMounts:
            - name: data
              mountPath: /data
            - name: config
              mountPath: /usr/local/etc/redis.conf
              subPath: redis.conf
          livenessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 60
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 5
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: redis
        - name: config
          configMap:
            name: redis
      nodeName: master
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        storageClassName: longhorn
        resources:
          requests:
            storage: 2Gi
apiVersion: apps/v1
kind: StatefulSet			
			]]>
			</screen>
		</section>
		<section>
			<title>nodeSelector 选择节点</title>
			<para>首先给节点打标签，例如 disk-type=ssd</para>
			<screen>
			<![CDATA[
[root@master ~]# kubectl label nodes agent-1 disk-type=ssd
node/agent-1 labeled			
			]]>
			</screen>
			<para>查看标签</para>
			<screen>
			<![CDATA[
[root@master ~]# kubectl get node --show-labels
NAME         STATUS   ROLES    AGE   VERSION   LABELS
master   Ready    master   42d   v1.17.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/master=
agent-1   Ready    <none>   42d   v1.17.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk-type=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=agent-1,kubernetes.io/os=linux
agent-2   Ready    <none>   42d   v1.17.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=agent-2,kubernetes.io/os=linux			
			]]>
			</screen>
			<para></para>
			<screen>
			<![CDATA[
apiVersion: apps/v1
kind: Deployment
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  replicas: 5
  selector:
    matchLabels:
      app: busybox
  template:
    metadata:
      labels:
        app: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        imagePullPolicy: IfNotPresent
        ports:
          - containerPort: 80
      # 指定标签节点
      nodeSelector:
        disk-type: ssd			
			]]>
			</screen>
			<para>删除标签</para>
			<screen>
			<![CDATA[
[root@master ~]# kubectl label nodes agent-1 disk-type-
node/agent-1 unlabeled		
			]]>
			</screen>
		</section>
		<section>
			<title>nodeAffinity 选择节点</title>
			<screen>
			<![CDATA[
nodeAffinity可对应的两种策略：
preferredDuringScheduling(IgnoredDuringExecution / RequiredDuringExecution) 软策略
requiredDuringScheduling(IgnoredDuringExecution / RequiredDuringExecution) 硬策略

operator 表达式
In: label的值在某个列表中
NotIn：label的值不在某个列表中
Exists：某个label存在
DoesNotExist：某个label不存在
Gt：label的值大于某个值（字符串比较）
Lt：label的值小于某个值（字符串比较）
			]]>
			</screen>
		</section>
		<section>
			<title>Taint（污点）和 Toleration（容忍）</title>
			<screen>
			<![CDATA[
			
			]]>
			</screen>
		</section>

	</section>

	<section id="Service">
		<title>Service</title>
		<section>
			<title>创建服务</title>
			<para>创建 service.yaml 文件</para>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
  - name: https
    protocol: TCP
    port: 443
    targetPort: 443
			]]>
			</screen>
			<para></para>
			<screen>
			<![CDATA[
iMac:kubernetes neo$ kubectl create -f service.yaml 
service/my-service created			
			]]>
			</screen>
			<para>查看服务</para>
			<screen>
			<![CDATA[
iMac:kubernetes neo$ kubectl get service
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          113m
my-service   ClusterIP   10.106.157.143   <none>        80/TCP,443/TCP   64s			
			]]>
			</screen>
			<para>查看 service 后端代理的 pod 的 ip，这里没有挂载 pod 所以显示 none</para>
			<screen>
			<![CDATA[
iMac:kubernetes neo$ kubectl get endpoints my-service
NAME         ENDPOINTS   AGE
my-service   <none>      2m20s			
			]]>
			</screen>
		</section>
		<section>
			<title>查看服务</title>

		</section>
		<section>
			<title>设置外部IP</title>
			<para>报漏 80.11.12.10:80 地址</para>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
  externalIPs:
    - 80.11.12.10			
			]]>
			</screen>
		</section>
		<section>
			<title>绑定外部域名</title>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com			
			]]>
			</screen>
		</section>
		<section>
			<title></title>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Service
metadata:
  name: spring-cloud-config-server
  namespace: default
  labels:
    app: springboot
spec:
  #type: NodePort
  ports: web
  - port: 8888
    targetPort: web
  clusterIP: 10.10.0.1
  selector:
    app: spring-cloud-config-server
			]]>
			</screen>
		</section>
		<section>
			<title>nodePort</title>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: MyApp
  ports:
      # By default and for convenience, the `targetPort` is set to the same value as the `port` field.
    - port: 80
      targetPort: 80
      # Optional field
      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)
      nodePort: 30007			
			]]>
			</screen>
		</section>
		<section>
			<title>LoadBalancer</title>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  clusterIP: 10.0.171.239
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 192.0.2.127			
			]]>
			</screen>
		</section>
		<section>
			<title>Example</title>
			<screen>
		<![CDATA[
apiVersion: v1
kind: Service
metadata:
  name: registry
  namespace: default
  labels:
    app: registry
spec:
  type: NodePort
  selector:
    app: registry
  ports:
  - name: registry
    port: 5000
    nodePort: 30050
    protocol: TCP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: registry
  namespace: default
  labels:
    app: registry
spec:
  replicas: 1
  selector:
    matchLabels:
      app: registry
  template:
    metadata:
      labels:
        app: registry
    spec:
      containers:
      - name: registry
        image: registry:latest
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        env:
        - name: REGISTRY_HTTP_ADDR
          value: :5000
        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
          value: /var/lib/registry
        ports:
        - containerPort: 5000
          name: registry
          protocol: TCP	
		]]>
			</screen>
		</section>
	</section>
	<section id="ConfigMap">
		<title>ConfigMap</title>
		<section>
			<title>Key-Value 配置</title>
			<screen>
			<![CDATA[
apiVersion: v1
kind: ConfigMap
metadata:
  name: db-config
  namespace: default
data:
  db.host: 172.16.0.10
  db.port: '3306'
  db.user: neo
  db.pass: chen
			]]>
			</screen>
			<para>创建配置</para>
			<screen>
			<![CDATA[
neo@MacBook-Pro-Neo ~/tmp/kubernetes % kubectl create -f key-value.yaml
configmap/db-config created
			]]>
			</screen>
			<para>将配置项保存到文件</para>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "cat /usr/local/etc/config/db.host" ]
      volumeMounts:
      - name: config-volume
        mountPath: /usr/local/etc/config
  volumes:
    - name: config-volume
      configMap:
        name: db-config
  restartPolicy: Never			
			]]>
			</screen>
			<para>定义多组配置项</para>
			<screen>
			<![CDATA[
apiVersion: v1
kind: ConfigMap
metadata:
  name: spring-cloud-config
  namespace: default
data:
  config: |
    spring.security.user=config
    spring.security.user=passw0rd
  euerka: |
    spring.security.user=eureka
    spring.security.user=passw0rd
  gateway: |
    spring.security.user=gateway
    spring.security.user=passw0rd    
			]]>
			</screen>
		</section>
		<section>
			<title>Secret</title>
			<para>制作私钥证书</para>
			<screen>
			<![CDATA[
openssl genrsa -out ingress.key 2048
			]]>
			</screen>
			<para>制作公钥证书</para>
			<screen>
			<![CDATA[
openssl req -new -x509 -days 3650 -key ingress.key -out ingress.crt						
			]]>
			</screen>
			<para>生成 BASE64</para>
			<screen>
			<![CDATA[
neo@MacBook-Pro-Neo ~/workspace/devops/demo % base64 ingress.crt 
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURhRENDQWxBQ0NRRFdsVG0x……
neo@MacBook-Pro-Neo ~/workspace/devops/demo % base64 ingress.key
LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVB……
			]]>
			</screen>
			<programlisting>
			<![CDATA[
apiVersion: v1
kind: Secret
metadata:
  name: tls
  namespace: development
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURhRENDQWxBQ0NRRFdsVG0x……
  tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVB……		
			]]>
			</programlisting>
		</section>
		<section>
			<title>环境变量</title>
			<para>envFrom 可将 ConfigMap 中的配置项定义为容器环境变量</para>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: neo-test-pod
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      envFrom:
      - configMapRef:
          name: special-config
  restartPolicy: Never			
			]]>
			</screen>
			<para>引用单个配置项使用 valueFrom</para>
			<screen>
			<![CDATA[
neo@MacBook-Pro-Neo ~/tmp/kubernetes % cat key-value.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: db-config
  namespace: default
data:
  db.host: 172.16.0.10
  db.port: '3306'		
  db.user: neo
  db.pass: chen
---
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: DBHOST
          valueFrom:
            configMapKeyRef:
              name: db-config
              key: db.host
        - name: DBPORT
          valueFrom:
            configMapKeyRef:
              name: db-config
              key: db.port
  restartPolicy: Never				
			
neo@MacBook-Pro-Neo ~/tmp/kubernetes % kubectl create -f key-value.yaml
configmap/db-config created
pod/test-pod created		
			]]>
			</screen>
		</section>
		<section>
			<title>配置文件</title>
			<para>定义配置</para>
			<screen>
		<![CDATA[
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-config
  labels:
    app: redis
data:
  redis.conf: |-
    pidfile /var/lib/redis/redis.pid
    dir /var/lib/redis
    port 6379
    bind 0.0.0.0
    appendonly yes
    protected-mode no
    requirepass 123456
		]]>
			</screen>
			<para>引用配置</para>
			<screen>
		<![CDATA[
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:5.0.8
        command: 
          - "sh"
          - "-c"
          - "redis-server /usr/local/etc/redis/redis.conf"
        ports:
        - containerPort: 6379
        resources:
          limits:
            cpu: 1000m
            memory: 1024Mi
          requests:
            cpu: 1000m
            memory: 1024Mi
        livenessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 300
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        readinessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        volumeMounts:
        - name: data
          mountPath: /data
        - name: config
          mountPath:  /usr/local/etc/redis/redis.conf
          subPath: redis.conf
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: redis
      - name: config
        configMap:
          name: redis-config
		]]>
			</screen>
			<screen>
			<![CDATA[
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh","-c","find /etc/config/" ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: path/to/special-key
  restartPolicy: Never			
			]]>
			</screen>
		</section>
	</section>
	<section id="Volume">
		<title>Volume</title>
		<screen>
		<![CDATA[
PersistentVolume 的访问模式（accessModes）有三种：

ReadWriteOnce（RWO）：是最基本的方式，可读可写，但只支持被单个节点挂载。
ReadOnlyMany（ROX）：可以以只读的方式被多个节点挂载。
ReadWriteMany（RWX）：这种存储可以以读写的方式被多个节点共享。不是每一种存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是 NFS。在 PVC 绑定 PV 时通常根据两个条件来绑定，一个是存储的大小，另一个就是访问模式。


PersistentVolume 的回收策略（persistentVolumeReclaimPolicy，即 PVC 释放卷的时候 PV 该如何操作）也有三种

Retain，不清理, 保留 Volume（需要手动清理）
Recycle，删除数据，即 rm -rf /thevolume/*（只有 NFS 和 HostPath 支持）
Delete，删除存储资源，比如删除 AWS EBS 卷（只有 AWS EBS, GCE PD, Azure Disk 和 Cinder 支持）		
		]]>
		</screen>
		<section>
			<title>local</title>
			<screen>
			<![CDATA[
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 100Gi
  # volumeMode field requires BlockVolume Alpha feature gate to be enabled.
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node			
			]]>
			</screen>
			<section>
				<title>案例</title>
				<screen>
				<![CDATA[
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-volume
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: netkiller-local-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-volume
  local:
    path: /tmp/neo
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - minikube
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: netkiller-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-volume
---
kind: Pod
apiVersion: v1
metadata:
  name: busybox
  namespace: default
spec:
  containers:
    - name: busybox
      image: busybox:latest
      # image: registry.netkiller.cn:5000/netkiller/welcome:latest
      imagePullPolicy: IfNotPresent
      command:
        - sleep
        - "3600"
      volumeMounts:
      - mountPath: "/srv"
        name: mypd
  restartPolicy: Always
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: netkiller-pvc			
				]]>
				</screen>
				<para>部署 POD</para>
				<screen>
				<![CDATA[
iMac:kubernetes neo$ kubectl create -f example/volume/local.yaml 
storageclass.storage.k8s.io/local-volume created
persistentvolume/netkiller-local-pv created
persistentvolumeclaim/netkiller-pvc created
pod/busybox created				
				]]>
				</screen>
				<para>查看POD状态</para>
				<screen>
				<![CDATA[
iMac:kubernetes neo$ kubectl get pod
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          2m28s				
				]]>
				</screen>
				<para>进入POD查看local卷的挂载情况，同时创建一个测试文件。</para>
				<screen>
				<![CDATA[
iMac:kubernetes neo$ kubectl exec -it busybox sh
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
/ # mount | grep /srv
tmpfs on /srv type tmpfs (rw)

/ # echo helloworld > /srv/netkiller
/ # cat /srv/netkiller 
helloworld
				]]>
				</screen>
				<para>进入宿主主机查看挂载目录</para>
				<screen>
				<![CDATA[
$ cat /tmp/neo/netkiller 
helloworld
				]]>
				</screen>
			</section>
		</section>
	</section>

	<section id="Job">
		<title>Job</title>
		<section>
			<title>执行单词任务</title>
			<para>.spec.completions 标志Job结束需要成功运行的Pod个数，默认为1</para>
			<para>.spec.parallelism 标志并行运行的Pod的个数，默认为1</para>
			<para>.spec.activeDeadlineSeconds 标志失败Pod的重试最大时间，超过这个时间不会继续重试</para>
			<screen>
			<![CDATA[
apiVersion: batch/v1
kind: Job
metadata:
  name: busybox
spec:
  completions: 1
  parallelism: 1
  template:
    metadata:
      name: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ["echo", "hello"]
      restartPolicy: Never			
			]]>
			</screen>
			<para></para>
			<screen>
			<![CDATA[
$ kubectl create -f job.yaml
job "busybox" created
$ pods=$(kubectl get pods --selector=job-name=busybox --output=jsonpath={.items..metadata.name})
$ kubectl logs $pods		
			]]>
			</screen>
		</section>
		<section>
			<title>计划任务</title>
			<para>.spec.schedule 指定任务运行周期，格式同Cron</para>
			<para>.spec.startingDeadlineSeconds 指定任务开始的截止期限</para>
			<para>.spec.concurrencyPolicy 指定任务的并发策略，支持Allow、Forbid和Replace三个选项</para>
			<screen>
			<![CDATA[
apiVersion: batch/v2alpha1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure			
			]]>
			</screen>
		</section>
	</section>
	<section id="Ingress">
		<title>Ingress</title>
		<para>正常情况 Service 只是暴露了端口，这个端口是可以对外访问的，但是80端口只有一个，很多 Service 都要使用 80端口，这时就需要使用虚拟主机技术。</para>
		<para>多个 Service 共同使用一个 80 端口，通过域名区分业务。这就是 Ingress 存在的意义。</para>
		<section>
			<title>端口</title>
			<screen>
			<![CDATA[
+----------+  Ingress   +---------+    Pod    +----------+
| internet | ---------> | Service | --------> | Pod Node |
+----------+            +---------+           +----------+
			
			]]>
			</screen>
			<screen>
			<![CDATA[
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: springboot
spec:
  backend:
    service:
      name: springboot
      port: 
        number: 80			
			]]>
			</screen>
		</section>
		<section>
			<title>URI 规则</title>
			<screen>
			<![CDATA[
                  Ingress	 / ---> /api --> api-service:8080		
www.netkiller.cn ---------> |  ---> /usr --> usr-service:8080
                             \ ---> /img --> img-service:8080			
			]]>
			</screen>
			<screen>
			<![CDATA[
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: uri-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: www.netkiller.cn
    http:
      paths:
      - path: /api
        backend:
          serviceName: api-service
          servicePort: 8080
      - path: /usr
        backend:
          serviceName: usr-service
          servicePort: 8080		
      - path: /img
        backend:
          serviceName: img-service
          servicePort: 8080		
			]]>
			</screen>
		</section>
		<section>
			<title>vhost 虚拟主机</title>
			<screen>
			<![CDATA[
www.netkiller.cn --|     Ingress     |-> www.netkiller.cn www:80
                   | --------------> |
img.netkiller.cn --|                 |-> img.netkiller.cn img:80			
			]]>
			</screen>
			<screen>
			<![CDATA[
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: vhost-ingress
spec:
  rules:
  - host: www.netkiller.cn
    http:
      paths:
      - backend:
          serviceName: www
          servicePort: 80
  - host: img.netkiller.cn
    http:
      paths:
      - backend:
          serviceName: img
          servicePort: 80			
			]]>
			</screen>
		</section>
		<section>
			<title>rewrite</title>
			<screen>
			<![CDATA[
http://www.netkiller.cn/1100 => /article/1100			
			]]>
			</screen>
			<screen>
			<![CDATA[
apiVersion: networking.k8s.io/v1beta1
kind: Ingress			
metadata:
  name: rewrite-ingress
  annotations: 
    nginx.ingress.kubernetes.io/rewrite-target: /article/$1
spec:
  rules:
  - host: www.netkiller.cn
    http:
      paths:
        # 可以有多个（可以正则）
        - path: /($/.*)
          backend:
            serviceName: article
            servicePort: 80	
			]]>
			</screen>
		</section>
		<section>
			<title>annotations 配置</title>
			<section>
				<title>HTTP 跳转到 HTTPS</title>
				<screen>
				<![CDATA[
# 该注解只在配置了HTTPS之后才会生效进行跳转
nginx.ingress.kubernetes.io/ssl-redirect: "true"

# 强制跳转到https，不论是否配置了https证书
nginx.ingress.kubernetes.io/force-ssl-redirect: "true"				
				]]>
				</screen>
			</section>
			<section>
				<title>server-snippet</title>
				<para>server-snippet 可以让你直接编排 Nginx 配置</para>
				<screen>
				<![CDATA[
nginx.ingress.kubernetes.io/server-snippet: |
    rewrite /api/($|.*) /api/v2/$1 break;
    rewrite /img/($|.*) /img/thumbnail/$1 break;
				]]>
				</screen>
			</section>

		</section>
		<section>
			<title>金丝雀发布（灰度发布）</title>
			<para>三种annotation按匹配优先级顺序：</para>
			<screen>
			<![CDATA[
canary-by-header > canary-by-cookie > canary-weight			
			]]>
			</screen>
			<section>
				<title>准备服务</title>
				<screen>
				<![CDATA[
# Release Version
apiVersion: v1
kind: Service
metadata:
    name: hello-service
    labels:
    app: hello-service
spec:
ports:
- port: 80
    protocol: TCP
selector:
    app: hello-service
---
# canary Version
apiVersion: v1
kind: Service
metadata:
    name: canary-hello-service
    labels:
    app: canary-hello-service
spec:
ports:
- port: 80
    protocol: TCP
selector:
    app: canary-hello-service				
				]]>
				</screen>
			</section>
			<section>
				<title>方案一，权重分配</title>
				<screen>
				<![CDATA[
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: canary
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "30"
spec:
  rules:
  - host: canary.netkiller.cn
    http:
      paths:
      - backend:
          serviceName: canary-hello-service 
				]]>
				</screen>
				<para></para>
				<screen>
				<![CDATA[
$ for i in $(seq 1 10); do curl http://canary.netkiller.cn; echo '\n'; done				
				]]>
				</screen>
			</section>
			<section>
				<title>通过HTTP头开启灰度发布</title>
				<screen>
				<![CDATA[
annotations:
  kubernetes.io/ingress.class: nginx
  nginx.ingress.kubernetes.io/canary: "true"
  nginx.ingress.kubernetes.io/canary-by-header: "canary"				
				]]>
				</screen>
				<para></para>
				<screen>
				<![CDATA[
$ for i in $(seq 1 5); do curl -H 'canary:always' http://canary.netkiller.cn; echo '\n'; done				
				]]>
				</screen>
				<para></para>
				<screen>
				<![CDATA[
annotations:
  kubernetes.io/ingress.class: nginx
  nginx.ingress.kubernetes.io/canary: "true"
  nginx.ingress.kubernetes.io/canary-by-header: "canary"
  nginx.ingress.kubernetes.io/canary-by-header-value: "true"				
				]]>
				</screen>
				<para></para>
				<screen>
				<![CDATA[
$ for i in $(seq 1 5); do curl -H 'canary:true' http://canary.netkiller.cn; echo '\n'; done						
				]]>
				</screen>
			</section>
			<section>
				<title>通过 Cookie 开启</title>
				<screen>
				<![CDATA[
annotations:
  kubernetes.io/ingress.class: nginx
  nginx.ingress.kubernetes.io/canary: "true"
  nginx.ingress.kubernetes.io/canary-by-cookie: "canary"				
				]]>
				</screen>
				<para></para>
				<screen>
				<![CDATA[
$ for i in $(seq 1 5); do curl -b 'canary=always' http://canary.netkiller.cn; echo '\n'; done				
				]]>
				</screen>
			</section>
		</section>

		<section>
			<title>管理 Ingress</title>
			<screen>
			<![CDATA[
# 查看已有配置
kubectl describe ingress test

# 修改配置
kubectl edit ingress test	

# 来重新载入配置
kubectl replace -f ingress.yaml		
			]]>
			</screen>
		</section>
	</section>
</section>