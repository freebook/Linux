<?xml version="1.0" encoding="UTF-8"?>
<!-- $Id: chapter.system.filesystem.xml 500 2012-12-04 09:01:55Z netkiller $ -->
<chapter id="index"><?dbhtml dir="filesystem" ?>
	<title>File System 文件系统</title>
	<section id="etc.fstab">
		<title>/etc/fstab</title>
		<screen>
		<![CDATA[
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
		]]>
		</screen>
		<para>mount point</para>
		<screen><![CDATA[
该字段描述希望的文件系统加载的目录，对于swap设备，该字段为none
		]]></screen>
		<para>file system</para>
		<screen><![CDATA[
例如/dev/cdrom或/dev/sdb,除了使用设备名，你可以使用设备的UUID或设备的卷标签，例如，LABAL=root 或 UUID=7f91104e-8187-4ccf-8215-6e2e641f32e3
		]]></screen>
		<para>type</para>
		<para>定义了该设备上的文件系统,系统可用文件系统</para>
		<screen><![CDATA[
$ cat /proc/filesystems
nodev   sysfs
nodev   rootfs
nodev   bdev
nodev   proc
nodev   cgroup
nodev   cpuset
nodev   tmpfs
nodev   devtmpfs
nodev   debugfs
nodev   securityfs
nodev   sockfs
nodev   pipefs
nodev   anon_inodefs
nodev   inotifyfs
nodev   devpts
        ext3
        ext2
        ext4
nodev   ramfs
nodev   hugetlbfs
nodev   ecryptfs
nodev   fuse
        fuseblk
nodev   fusectl
nodev   mqueue
nodev   rpc_pipefs
nodev   nfs
nodev   nfs4
        reiserfs
        xfs
        jfs
        msdos
        vfat
        ntfs
        minix
        hfs
        hfsplus
        qnx4
        ufs
        btrfs
        iso9660

		]]></screen>
		<para>options</para>
		<screen><![CDATA[
选项　　　　　　　　　　　　　　含义
defaults  使用默认设置。	等于rw,suid,dev,exec,auto,nouser,async，

rw   挂载为读写权限
ro　　　　以只读模式加载该文件系统

exec    是一个默认设置项，它使在那个分区中的可执行的二进制文件能够执行。
noexec	二进制文件不允许执行。

sync　　　不对该设备的写操作进行缓冲处理，这可以防止在非正常关机时情况下破坏文件系统，但是却降低了计算机速度
async  	所有的I/O将以异步方式进行

user　　　允许普通用户加载该文件系统
nouser  只允许root用户挂载。这是默认设置。

quota　　　强制在该文件系统上进行磁盘定额限制
noauto　　不再使用mount -a命令（例如系统启动时）加载该文件系统

noatime/nodiratime	禁止更新访问时间

		]]></screen>
		<para>dump</para>
		<screen><![CDATA[
dump　-　该选项被"dump"命令使用来检查一个文件系统应该以多快频率进行转储，若不需要转储就设置该字段为0
		]]></screen>
		<para>pass</para>
		<screen><![CDATA[
该字段被fsck命令用来决定在启动时需要被扫描的文件系统的顺序，根文件系统"/"对应该字段的值应该为1，其他文件系统应该为2。若该文件系统无需在启动时扫描则设置该字段为0
		]]></screen>
		<para>noatime/nodiratime</para>
		<screen><![CDATA[
/dev/sda2 /data ext3 defaults 0 2
/dev/sda2 /data ext3 defaults,noatime,nodiratime 0 2
		]]></screen>
		<screen><![CDATA[
mount -o remount /data
mount -o noatime -o nodiratime -o remount /data
		]]></screen>
		<section>
			<title>绑定目录</title>
			<para>/etc/fstab 中添加</para>
			<screen>
			<![CDATA[
/opt/storage /var/lib/rancher/k3s/storage none defaults,bind 0 0			
			]]>
			</screen>
			<para>使用 lsblk 查看挂载情况</para>
			<screen>
			<![CDATA[
[root@master ~]# lsblk -a
NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
sda           8:0    0 931.5G  0 disk 
`-sda1        8:1    0 931.5G  0 part /var/lib/rancher/k3s/storage
                                      /opt
nvme0n1     259:0    0 238.5G  0 disk 
|-nvme0n1p1 259:1    0   600M  0 part /boot/efi
|-nvme0n1p2 259:2    0     1G  0 part /boot
|-nvme0n1p3 259:3    0    64G  0 part [SWAP]
`-nvme0n1p4 259:4    0 172.9G  0 part /			
			]]>
			</screen>
		</section>
		<section>
			<title>禁止执行</title>
			<para>验证 noexec</para>
			<screen>
			<![CDATA[ 
root@logging ~# cd /opt/log/
root@logging /o/log# echo ls > dir.sh
root@logging /o/log# chmod +x dir.sh
root@logging /o/log# ./dir.sh
fish: The file “./dir.sh” is not executable by this user
			]]>
			</screen>
		</section>
		<section>
			<title>禁止更新访问时间</title>
			<screen>
			<![CDATA[ 
root@logging ~# touch netkiller.txt
root@logging ~# stat netkiller.txt
	File: netkiller.txt
	Size: 0         	Blocks: 0          IO Block: 4096   regular empty file
Device: fd03h/64771d	Inode: 816         Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2023-01-13 15:27:48.282376191 +0800
Modify: 2023-01-13 15:27:48.282376191 +0800
Change: 2023-01-13 15:27:48.282376191 +0800
	Birth: 2023-01-13 15:27:48.282376191 +0800
root@logging ~# cat netkiller.txt
root@logging ~# stat netkiller.txt
	File: netkiller.txt
	Size: 0         	Blocks: 0          IO Block: 4096   regular empty file
Device: fd03h/64771d	Inode: 816         Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2023-01-13 15:28:00.979854433 +0800
Modify: 2023-01-13 15:27:48.282376191 +0800
Change: 2023-01-13 15:27:48.282376191 +0800
	Birth: 2023-01-13 15:27:48.282376191 +0800		
			]]>
			</screen>
			<para>加入 noatime,nodiratime </para>
			<screen>
			<![CDATA[ 
root@logging ~# cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Mon Nov 21 02:06:17 2022
#
# Accessible filesystems, by reference, are maintained under '/dev/disk/'.
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info.
#
# After editing this file, run 'systemctl daemon-reload' to update systemd
# units generated from this file.
#
UUID=16ca8836-7ca9-454f-9a72-8efbae5edc51 /                       xfs     defaults        0 0
UUID=D168-FFBD          /boot/efi               vfat    defaults,uid=0,gid=0,umask=077,shortname=winnt 0 2
UUID=ec48f3c2-80c8-4ed1-be56-049a95c2b60e	/opt/log	xfs noatime,nodiratime,noexec 0 0		
			]]>
			</screen>
			
			<para>验证 noatime,nodiratime</para>
			<screen>
			<![CDATA[ 
root@logging /o/log# echo Helloworld > neo.txt

root@logging /o/log# stat neo.txt
	File: neo.txt
	Size: 11        	Blocks: 8          IO Block: 4096   regular file
Device: fd11h/64785d	Inode: 141         Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2023-01-13 15:37:47.375940824 +0800
Modify: 2023-01-13 15:37:47.375940824 +0800
Change: 2023-01-13 15:37:47.375940824 +0800
Birth: 2023-01-13 15:37:47.375940824 +0800

root@logging /o/log# cat neo.txt
Helloworld

root@logging /o/log# stat neo.txt
	File: neo.txt
	Size: 11        	Blocks: 8          IO Block: 4096   regular file
Device: fd11h/64785d	Inode: 141         Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2023-01-13 15:37:47.375940824 +0800
Modify: 2023-01-13 15:37:47.375940824 +0800
Change: 2023-01-13 15:37:47.375940824 +0800
Birth: 2023-01-13 15:37:47.375940824 +0800		
			]]>
			</screen>
		</section>
		<section>
			<title>/etc/fstab 例子</title>
			<para>/etc/fstab btrfs 实例</para>
			<screen>
			<![CDATA[
neo@netkiller:~$ cat /etc/fstab 
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
# / was on /dev/sda1 during installation
UUID=d103e33f-7f9f-4911-918e-32eae42e229c /               btrfs   defaults,subvol=@ 0       1
# /home was on /dev/sda1 during installation
UUID=d103e33f-7f9f-4911-918e-32eae42e229c /home           btrfs   defaults,subvol=@home 0       2
# /opt was on /dev/sda6 during installation
UUID=63d0b776-3bbd-490f-8284-f148b255185e /opt            btrfs   noatime,nodiratime,noexec 0       2
# swap was on /dev/sda5 during installation
UUID=ff8945bf-fa45-49e5-b3d2-bb833bc6dc9c none            swap    sw              0       0
			]]>
			</screen>
			<para>背景如下：</para>
			<para>我们的服务器通常有一个系统盘，用来安装操作系统，再挂在一个数据盘用来存储数据，这个数据盘有时是机械硬盘，为了提高IO性能，我们通常会禁止atime，为了提高安全性，我们还会禁止创建可执行文件。</para>
			<para>noatime 禁止更新访问时间, nodiratime 禁止更新目录访问时间, noexec 禁止创建可执行文件</para>
			<screen>
			<![CDATA[ 
root@logging ~# cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Mon Nov 21 02:06:17 2022
#
# Accessible filesystems, by reference, are maintained under '/dev/disk/'.
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info.
#
# After editing this file, run 'systemctl daemon-reload' to update systemd
# units generated from this file.
#
UUID=16ca8836-7ca9-454f-9a72-8efbae5edc51 /                       xfs     defaults        0 0
UUID=D168-FFBD          /boot/efi               vfat    defaults,uid=0,gid=0,umask=077,shortname=winnt 0 2
UUID=ec48f3c2-80c8-4ed1-be56-049a95c2b60e	/opt/log	xfs noatime,nodiratime,noexec 0 0		
			]]>
			</screen>
		</section>
	</section>
	<section id="mount">
		<title>Mount partition</title>
		<section>
			<title>Mount</title>
			<screen><![CDATA[
sudo mount /dev/sdb1 /mnt/mount1
			]]></screen>
			<para>支持UTF-8</para>
			<screen><![CDATA[
mount -o iocharset=utf8 /dev/sda5 /mnt/usb
			]]></screen>
			<para>禁止文件与目录的访问时间</para>
			<screen><![CDATA[
mount -o noatime,nodiratime /dev/drbd0  /mnt
			]]></screen>
		</section>
		<section>
			<title>Umount</title>
			<para>umount - unmount file systems</para>
			<screen><![CDATA[
sudo umount /mnt/mount1
			]]></screen>
		</section>
		<section>
			<title>bind directory</title>
			<para></para>
			<screen><![CDATA[
mount --bind /foo /home/neo/foo
			]]></screen>
			<para>挂载目录将不能被删除，但目录下文件可以删除</para>
			<screen><![CDATA[
# rm -rf /home/neo/foo
rm: cannot remove directory '/home/neo/foo': Device or resource busy
			]]></screen>
			<para>/etc/fstab</para>
			<screen><![CDATA[
/foo /home/neo/foo    none    bind    0 0
			]]></screen>
		</section>

	</section>
	<section id="hdd.format">
		<title>ext2</title>
		<screen><![CDATA[
neo@netkiller:~# mkfs.ext2 /dev/sdb1		
		]]></screen>
		<para>ext2 是早期Linux使用的文件系统存在很多缺陷，建议不要在使用。</para>
	</section>
	<section id="ext3">
		<title>ext3</title>
		<para>format /dev/sdb1</para>
		<screen><![CDATA[
neo@netkiller:~$ sudo mkfs.ext3 /dev/sdb1
		]]></screen>
	</section>
	<section id="reiserfs">
		<title>ReiserFS</title>
		<para>you also can using other file system</para>
		<para>reiserfs</para>
		<screen><![CDATA[
neo@netkiller:~$ sudo mkfs.reiserfs /dev/sdb1
		]]></screen>
	</section>

	<section id="ext4">
		<title>EXT4</title>
		<section>
			<title>install</title>
			<screen><![CDATA[
# yum install e4fsprogs
			]]></screen>
		</section>
		<section>
			<title>format</title>
			<screen><![CDATA[
# mkfs.ext4 /dev/sda2
			]]></screen>
		</section>
		<section>
			<title>label</title>
			<screen><![CDATA[
# e4label /dev/sda2 /www

# mkdir /www

# cat /etc/fstab |grep ext4
LABEL=/www              /www                    ext4    defaults        1 2
			]]></screen>
		</section>
		<section>
			<title>mount/umount</title>
			<screen><![CDATA[
# mount /www
# umount /www
			]]></screen>
		</section>
		<section>
			<title>LVM 卷</title>
			<screen><![CDATA[
# mkfs.ext4 /dev/VolGroup00/LogVol02

[root@images ~]# cat /etc/fstab
/dev/VolGroup00/LogVol00 /                       ext3    defaults        1 1
/dev/VolGroup00/LogVol02 /images                 ext4    defaults        1 2
LABEL=/boot             /boot                   ext3    defaults        1 2
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
/dev/VolGroup00/LogVol01 swap                    swap    defaults        0 0

# mount -a
			]]></screen>
		</section>
	</section>
	<section id="lvm">
		<title>LVM</title>
		<para>请参考《Netkiller Storage 手札》·LVM相关章节</para>
	</section>
	<section id="index"><?dbhtml dir="btrfs" ?>
		<title>Btrfs</title>
		<para>The btrfs utility is a toolbox for managing btrfs filesystems. There are command groups to work with subvolumes, devices, for whole filesystem or other specific actions.</para>
		<section id="fstab">
			<title>/etc/fstab</title>
			<screen><![CDATA[
[root@netkiller ~]# cat /etc/fstab 

#
# /etc/fstab
# Created by anaconda on Fri Nov 21 18:16:53 2014
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
UUID=6634633e-001d-43ba-8fab-202f1df93339 / ext4 defaults,barrier=0 1 1
UUID=786f570d-fe5c-4d5f-832a-c1b0963dd4e6 /srv btrfs defaults 1 1
UUID=786f570d-fe5c-4d5f-832a-c1b0963dd4e6 /var/lib/mongo  btrfs   noatime,nodiratime,subvol=@mongo 0 2
UUID=786f570d-fe5c-4d5f-832a-c1b0963dd4e6 /var/lib/mysql  btrfs   noatime,nodiratime,subvol=@mysql 0 2
UUID=786f570d-fe5c-4d5f-832a-c1b0963dd4e6 /www  btrfs   noatime,nodiratime,subvol=www 0 2
		]]></screen>
			<screen><![CDATA[
[root@netkiller ~]# df -T
Filesystem     Type     1K-blocks    Used Available Use% Mounted on
/dev/vda1      ext4      41151808 4902608  34135768  13% /
devtmpfs       devtmpfs   8127268       0   8127268   0% /dev
tmpfs          tmpfs      8133908       4   8133904   1% /dev/shm
tmpfs          tmpfs      8133908  849468   7284440  11% /run
tmpfs          tmpfs      8133908       0   8133908   0% /sys/fs/cgroup
/dev/vdb1      btrfs    104856576 1404116 101369420   2% /srv
/dev/vdb1      btrfs    104856576 1404116 101369420   2% /var/lib/mongo
/dev/vdb1      btrfs    104856576 1404116 101369420   2% /var/lib/mysql
tmpfs          tmpfs      1626784       0   1626784   0% /run/user/0
/dev/vdb1      btrfs    104856576 1404116 101369420   2% /www
		]]></screen>
			<screen><![CDATA[
[root@netkiller ~]# ll /srv/
total 0
drwxr-xr-x 1 www    www    132 2016-12-08 15:17 apache-tomcat-8.5.8
drwxr-xr-x 1 root   root   132 2016-12-08 16:11 apache-tomcat-www
drwxr-xr-x 1 mongod mongod 608 2016-12-26 09:18 @mongo
drwxr-x--x 1 mysql  mysql  448 2016-12-26 09:16 @mysql
drwxr-xr-x 1 root   root     0 2016-11-30 13:24 @redis
drwxr-xr-x 1 root   root    22 2016-12-08 16:32 sbin
drwxr-xr-x 1 www    www     24 2016-12-08 16:54 www
		]]></screen>
		</section>
		<section id="format">
			<title>btrfs</title>
			<screen><![CDATA[
yum install btrfs-progs
		]]></screen>
			<screen><![CDATA[
# mkfs.btrfs /dev/sdb1
		]]></screen>
			<para>指定卷标</para>
			<screen><![CDATA[
# mkfs.btrfs /dev/sdb2 -L /backup
		]]></screen>
		</section>
		<section id="mount">
			<title>Mount Btrfs</title>
			<screen><![CDATA[
# mkdir /mnt/btrfs
# mount /dev/sdb1 /mnt/btrfs
		]]></screen>
			<para>查看挂载是否成功</para>
			<screen><![CDATA[
# df -Th
Filesystem    Type    Size  Used Avail Use% Mounted on
/dev/sda1     ext4     49G   15G   32G  32% /
tmpfs        tmpfs     32G  264K   32G   1% /dev/shm
/dev/sda3     ext4     52G  1.3G   48G   3% /var
/dev/sdb1    btrfs    2.0T   14G  2.0T   1% /mnt/btrfs
		]]></screen>
			<screen><![CDATA[
 针对 SSD 的优化:
# mount –t btrfs –o SSD /dev/sda5 /btrfsdisk


打开压缩功能：
# mount –t btrfs –o compress /dev/sda5 /btrfsdisk
		]]></screen>
			<section id="mount.snap">
				<title>Mount Snap</title>
				<para>mount -t btrfs -o subvol=your_snapshot /dev/sdb2 /mnt/snap</para>
				<screen><![CDATA[
mount -t btrfs -o subvol=aaa /dev/md127p5 /mnt/snap
			]]></screen>
			</section>
			<section id="btrfs.fstab">
				<title>fstab</title>
				<section id="btrfs-show">
					<title>btrfs-show</title>
					<screen><![CDATA[
[root@r610 ~]# btrfs-show
Label: none  uuid: 0b097eeb-1f0b-476a-955b-52122ef42bfc
        Total devices 1 FS bytes used 13.03GB
        devid    1 size 2.00TB used 24.04GB path /dev/sdb1

Btrfs Btrfs v0.19
				]]></screen>
				</section>
				<section>
					<title>/etc/fstab</title>
					<screen><![CDATA[
UUID=0b097eeb-1f0b-476a-955b-52122ef42bfc /opt    btrfs   defaults 1 2
				]]></screen>
				</section>
			</section>
		</section>
		<section id="subvolumes">
			<title>subvolumes</title>
			<screen><![CDATA[
# df -T
Filesystem     Type  1K-blocks      Used Available Use% Mounted on
/dev/md126p2   ext4   50395844  19952780  27883064  42% /
tmpfs          tmpfs   4024944       800   4024144   1% /dev/shm
/dev/md126p1   ext4     495844    172140    298104  37% /boot
/dev/md126p6   btrfs 500084736 360835636 119893924  76% /opt
/dev/md126p5   btrfs 409600000  24927332 368284612   7% /www

# btrfs subvolume create /www/git
Create subvolume '/www/git'

# btrfs subvolume list /www
ID 641 gen 21351 top level 5 path git
			]]></screen>
			<para>fstab 挂在子卷</para>
			<screen><![CDATA[
$ cat /etc/fstab 

#
# /etc/fstab
# Created by anaconda on Thu Oct 18 13:53:45 2012
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
UUID=88ec1ccf-7d8d-4107-a143-1ed0ec64a572 /                       ext4    defaults        1 1
UUID=c0786771-1c85-45be-a9ab-ef3ee16fccb4 /boot                   ext4    defaults        1 2
UUID=e1b89740-21f0-4507-97e9-a658cd7d3716 /opt                    btrfs   defaults        1 2
UUID=76e46795-ebaf-4d2d-8996-1e15979bf3c8 /www                    btrfs   defaults        1 2
UUID=76e46795-ebaf-4d2d-8996-1e15979bf3c8 /home/git               btrfs   defaults,subvol=git        1 2
UUID=c578f1b3-4bbe-4f48-b3d3-3929c65cb99c swap                    swap    defaults        0 0
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
		]]></screen>
		</section>
		<section id="snapshot">
			<title>snapshot</title>
			<screen><![CDATA[
创建快照
# btrfs subvolume snapshot /www /www/backup_2012
查看快照
# btrfs subvolume list -a /www
挂在快照
# mount -t btrfs -o subvol=backup_2012 /dev/md127p5 /mnt/snap
删除快照
# btrfs subvolume delete /www/backup_2012
Delete subvolume '/www/backup_2012'
		]]></screen>
		</section>
		<section id="btrfsctl">
			<title>btrfsctl</title>
			<section id="btrfs.resizes">
				<title>Resizes the filesystem</title>

			</section>
			<section id="btrfs.snapshot">
				<title>Snapshot</title>
				<para>Btrfs v0.19</para>
				<screen><![CDATA[
# touch /mnt/btrfs/test1
# touch /mnt/btrfs/test2
# ls /mnt/btrfs/test?
/mnt/btrfs/test1  /mnt/btrfs/test2
				]]></screen>
				<screen><![CDATA[
# echo 'This is a test' > /mnt/btrfs/test1
# btrfsctl –s snap1 /mnt/btrfs
#vi test1
 Test1 is modified
#cd /mnt/btrfs/snap1
#cat test1
 This is a test
			]]></screen>
			</section>
		</section>
		<section id="btrfs-vol">
			<title>btrfs-vol</title>
			<screen><![CDATA[
# btrfs-vol –a /dev/sdc1 /mnt/btrfs
		]]></screen>
		</section>
		<section id="btrfs-convert">
			<title>btrfs-convert</title>
			<screen><![CDATA[
btrfs-convert /dev/sdb1
		]]></screen>
		</section>
		<section id="btrfsck">
			<title>btrfsck</title>
			<screen><![CDATA[
# btrfsck /dev/sdb1
found 13994164224 bytes used err is 0
total csum bytes: 13588316
total tree bytes: 79728640
total fs tree bytes: 28860416
btree space waste bytes: 10282024
file data blocks allocated: 13931024384
 referenced 13906980864
Btrfs Btrfs v0.19
		]]></screen>
		</section>
		<section id="btrfs-debug-tree">
			<title>btrfs-debug-tree</title>
			<screen><![CDATA[
[root@r610 ~]# btrfs-debug-tree /dev/sdb1 |head
root tree
leaf 49463296 items 9 free space 2349 generation 298 owner 1
fs uuid 0b097eeb-1f0b-476a-955b-52122ef42bfc
chunk uuid 2826f868-c775-4835-8690-1020a2a9fbf5
        item 0 key (EXTENT_TREE ROOT_ITEM 0) itemoff 3756 itemsize 239
                root data bytenr 49446912 level 2 dirid 0 refs 1
        item 1 key (DEV_TREE ROOT_ITEM 0) itemoff 3517 itemsize 239
                root data bytenr 36139008 level 0 dirid 0 refs 1
        item 2 key (FS_TREE INODE_REF 6) itemoff 3500 itemsize 17
                inode ref index 0 namelen 7 name: default

		]]></screen>
		</section>
	</section>

	<section id="zfs">
		<title>zfs</title>
		<screen><![CDATA[
# yum info zfs-fuse

Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * addons: mirrors.163.com
 * base: mirrors.163.com
 * epel: mirror01.idc.hinet.net
 * extras: mirrors.163.com
 * updates: mirrors.163.com
Available Packages
Name       : zfs-fuse
Arch       : x86_64
Version    : 0.6.9_beta3
Release    : 0.el5
Size       : 1.5 M
Repo       : epel
Summary    : ZFS ported to Linux FUSE
URL        : http://zfs-fuse.net/
License    : CDDL
Description: ZFS is an advanced modern general-purpose filesystem from Sun
           : Microsystems, originally designed for Solaris/OpenSolaris.
           :
           : This project is a port of ZFS to the FUSE framework for the Linux
           : operating system.
           :
           : Project home page is at http://zfs-fuse.net/
		]]></screen>
	</section>


	<section id="iscsi">
		<title>iSCSI</title>
		<para>iSCSI 需要与GFS配合使用，其他文件系统不能实现数据同步。</para>
		<procedure>
			<title>iSCSI Example</title>
			<step>
				<para>install.</para>
				<screen><![CDATA[
# yum install iscsi-initiator-utils -y
# rpm -ql iscsi-initiator-utils
# rpm -q --scripts iscsi-initiator-utils
postinstall scriptlet (using /bin/sh):
/sbin/ldconfig

if [ ! -f /etc/iscsi/initiatorname.iscsi ]; then
        echo "InitiatorName=`/sbin/iscsi-iname`" > /etc/iscsi/initiatorname.iscsi
fi
/sbin/chkconfig --add iscsid
/sbin/chkconfig --add iscsi
preuninstall scriptlet (using /bin/sh):
if [ "$1" = "0" ]; then
    /sbin/chkconfig --del iscsi
    /sbin/chkconfig --del iscsid
fi
postuninstall scriptlet (using /bin/sh):
/sbin/ldconfig
				]]></screen>
			</step>
			<step>
				<para>config</para>
				<screen><![CDATA[
# cat /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.1994-05.com.redhat:9b2024102698
				]]></screen>
			</step>
			<step>
				<para>starting service.</para>
				<screen><![CDATA[
# chkconfig iscsi on
# chkconfig iscsid on

# service iscsi start
iscsid is stopped
Starting iSCSI daemon:                                     [  OK  ]
                                                           [  OK  ]
Setting up iSCSI targets: iscsiadm: No records found!
                                                           [  OK  ]
# service iscsi status
iscsid (pid  17501) is running...
# service iscsid status
iscsid (pid  17501) is running...

				]]></screen>
			</step>

			<step>
				<para>discovery targets.</para>
				<screen><![CDATA[
# iscsiadm -m discovery -t sendtargets -p 172.16.0.30:3260
172.16.0.30:3260,1 iqn.2010-09.com.openfiler:tsn.c7a241688f35
				]]></screen>
				<para>or</para>
				<screen><![CDATA[
iscsiadm --mode discovery --type sendtargets --portal 172.16.0.30:3260

iscsiadm -m discovery -t st -p 172.16.0.30:3260

				]]></screen>
			</step>
			<step>
				<para>login / logout</para>
				<screen><![CDATA[
# iscsiadm -m node --loginall=all
Logging in to [iface: default, target: iqn.2010-09.com.openfiler:tsn.c7a241688f35, portal: 172.16.0.30,3260]
Login to [iface: default, target: iqn.2010-09.com.openfiler:tsn.c7a241688f35, portal: 172.16.0.30,3260]: successful
				]]></screen>
				<para>or</para>
				<screen><![CDATA[
iscsiadm --mode node --targetname iqn.2010-09.com.openfiler:tsn.c7a241688f35 --portal 192.168.0.10:3260 --login
				]]></screen>
				<para>logout</para>
				<screen><![CDATA[
# iscsiadm -m node --logoutall=all
				]]></screen>
			</step>
			<step>
				<para>分区设置</para>
				<screen><![CDATA[
fdisk -l
fdisk /dev/sdb #依次选p n 1 w
mkfs.ext4 /dev/sdb1

挂载
mkdir /iscsi
mount /dev/sdb1 /iscsi

设自动挂载
vi /etc/fstab
/dev/sdb1 /iscsi ext3 _netdev 0 0
				]]></screen>
			</step>
		</procedure>
		<para>auth</para>
		<screen><![CDATA[
# cp /etc/iscsi/iscsid.conf /etc/iscsi/iscsid.conf.old
# vim /etc/iscsi/iscsid.conf

		]]></screen>
		<para>show node</para>
		<screen><![CDATA[
]# iscsiadm -m node
172.16.0.30:3260,1 iqn.2006-01.com.openfiler:tsn.0b232d1cc3ee
172.16.0.30:3260,1 iqn.2010-09.com.openfiler:tsn.c7a241688f35

		]]></screen>
		<para>delete node</para>
		<screen><![CDATA[
iscsiadm -m node -o delete -T iqn.2006-01.com.openfiler:tsn.0b232d1cc3ee
		]]></screen>
		<section>
			<title>GFS</title>
			<screen><![CDATA[
[root@dev2 ~]# /etc/init.d/iscsi start
iscsid is stopped
Starting iSCSI daemon:                                     [  OK  ]
                                                           [  OK  ]
Setting up iSCSI targets: iscsiadm: No records found!
                                                           [  OK  ]
[root@dev2 ~]# iscsiadm -m discovery -t st -p 192.168.3.194
192.168.3.194:3260,1 iqn.2007-09.jp.ne.peach.istgt:disk0
[root@dev2 ~]# iscsiadm -m node -l
Logging in to [iface: default, target: iqn.2007-09.jp.ne.peach.istgt:disk0, portal: 192.168.3.194,3260]
Login to [iface: default, target: iqn.2007-09.jp.ne.peach.istgt:disk0, portal: 192.168.3.194,3260]: successful
			]]></screen>

			<screen><![CDATA[
# fdisk -l

Disk /dev/sda: 250.0 GB, 250000000000 bytes
255 heads, 63 sectors/track, 30394 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *           1          13      104391   83  Linux
/dev/sda2              14       30394   244035382+  8e  Linux LVM

Disk /dev/sdb: 499.5 GB, 499558383616 bytes
255 heads, 63 sectors/track, 60734 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes

Disk /dev/sdb doesn't contain a valid partition table





fdisk /dev/sdb


# fdisk -l /dev/sdb

Disk /dev/sdb: 499.5 GB, 499558383616 bytes
255 heads, 63 sectors/track, 60734 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1               1       60734   487845823+   5  Extended
/dev/sdb5               1       60734   487845792   83  Linux


# mkfs.gfs2 -p lock_dlm -t edb_ha:gfs1 -j 3 /dev/sdb5
This will destroy any data on /dev/sdb5.

Are you sure you want to proceed? [y/n] y

Device:                    /dev/sdb5
Blocksize:                 4096
Device Size                465.25 GB (121961448 blocks)
Filesystem Size:           465.25 GB (121961446 blocks)
Journals:                  3
Resource Groups:           1861
Locking Protocol:          "lock_dlm"
Lock Table:                "edb_ha:gfs1"
UUID:                      A75C4963-85A2-A28B-4099-07FD7E3379D6
			]]></screen>
		</section>
	</section>
	<section id="gfs">
		<title>GFS - Cluster Storage</title>
		<para>
			<ulink url="http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Global_File_System_2/index.html" />
		</para>
		<screen><![CDATA[
yum groupinstall "Cluster Storage"
# egrep 'GFS2|DLM' /boot/config-2.6.32-*
/boot/config-2.6.32-131.2.1.el6.x86_64:CONFIG_GFS2_FS=m
/boot/config-2.6.32-131.2.1.el6.x86_64:CONFIG_GFS2_FS_LOCKING_DLM=y
/boot/config-2.6.32-131.2.1.el6.x86_64:CONFIG_DLM=m
/boot/config-2.6.32-131.2.1.el6.x86_64:CONFIG_DLM_DEBUG=y
/boot/config-2.6.32-71.el6.x86_64:CONFIG_GFS2_FS=m
/boot/config-2.6.32-71.el6.x86_64:CONFIG_GFS2_FS_LOCKING_DLM=y
/boot/config-2.6.32-71.el6.x86_64:CONFIG_DLM=m
/boot/config-2.6.32-71.el6.x86_64:CONFIG_DLM_DEBUG=y
		]]></screen>
		<screen><![CDATA[
pvcreate /dev/sdb3
vgcreate vg01 /dev/sdb3
lvcreate -L 500G -n lvol0 vg01
mkfs.gfs2 -p lock_dlm -t alpha:mydata1 -j 8 /dev/vg01/lvol0
		]]></screen>

		<screen><![CDATA[
# pvcreate /dev/sdb3
  Physical volume "/dev/sdb3" successfully created
# vgcreate vg01 /dev/sdb3
  Volume group "vg01" successfully created
# lvcreate -L 500G -n lvol0 vg01
  Logical volume "lvol0" created


# mkfs.gfs2 -p lock_dlm -t alpha:mydata1 -j 8 /dev/vg01/lvol0
This will destroy any data on /dev/vg01/lvol0.
It appears to contain: symbolic link to `../dm-6'

Are you sure you want to proceed? [y/n] y

Device:                    /dev/vg01/lvol0
Blocksize:                 4096
Device Size                500.00 GB (131072000 blocks)
Filesystem Size:           500.00 GB (131071998 blocks)
Journals:                  8
Resource Groups:           2000
Locking Protocol:          "lock_dlm"
Lock Table:                "alpha:mydata1"
UUID:                      4FC256A0-00BD-2087-15DF-8EA4366481AA
		]]></screen>
		<screen><![CDATA[
# mkdir /mnt/gfs2
# mount -t gfs2 -o noatime /dev/mapper/vg01-lvol0 /mnt/gfs2
gfs_controld join connect error: Connection refused
error mounting lockproto lock_dlm
		]]></screen>
	</section>
	<section id="glusterfs">
		<title>glusterfs</title>
		<screen><![CDATA[
# rpm -Uvh http://download.fedora.redhat.com/pub/epel/6/x86_64/epel-release-6-5.noarch.rpm
# yum install glusterfs glusterfs-fuse glusterfs-rdma glusterfs-server glusterfs-vim

# gzip /etc/glusterfs/*
		]]></screen>
		<screen><![CDATA[
# chkconfig glusterd off
# chkconfig glusterfsd on

# service glusterd stop
# service glusterfsd start

or

/etc/init.d/glusterd start
/etc/init.d/glusterfsd start
		]]></screen>

		<screen>
		<![CDATA[
mkdir -p /home/export

cat >> /etc/hosts <<EOF
192.168.80.107  gluster1
192.168.80.33   gluster2
192.168.80.1    gluster3
EOF

# glusterfs-volgen --name store1 --raid 1 gluster1:/home/export gluster2:/home/export
Generating server volfiles.. for server gluster2 as '/root/gluster2-store1-export.vol'
Generating server volfiles.. for server gluster1 as '/root/gluster1-store1-export.vol'
Generating client volfiles.. '/root/store1-tcp.vol'

cp ./store1-tcp.vol /etc/glusterfs/glusterfs.vol
scp gluster1-store1-export.vol root@gluster1:/etc/glusterfs/glusterfsd.vol
scp gluster2-store1-export.vol root@gluster2:/etc/glusterfs/glusterfsd.vol

ssh root@gluster1 service glusterfsd restart
ssh root@gluster2 service glusterfsd restart

# mkdir -p /mnt/glusterfs
# glusterfs /mnt/glusterfs/
or
# glusterfs -l /tmp/glusterfs.log -f /etc/glusterfs/glusterfs.vol /mnt/glusterfs/
        ]]>
		</screen>
		<para>Umount</para>
		<screen>
        <![CDATA[
umount /mnt/glusterfs
        ]]>
		</screen>
		<para>注意：请使用umount 卸载，不要kill glusterfs进程</para>
	</section>

	<section id="ramfs">
		<title>RAM FS</title>
		<screen><![CDATA[
# mkdir -p /mnt/ram1
# mount -t ramfs none /mnt/ram1 -o maxsize=10000
	]]></screen>
	</section>
	<section id="tmpfs">
		<title>tmpfs</title>
		<screen><![CDATA[
# mkdir -p /mnt/tmpfs
# mount tmpfs /mnt/tmpfs -t tmpfs
# mount tmpfs /mnt/tmpfs -t tmpfs -o size=32m
	]]></screen>
	</section>
	<section id="curlftpfs">
		<title>ftp fs</title>
		<para>安装</para>
		<screen><![CDATA[
sudo apt-get install curlftpfs
	]]></screen>
		<para>挂载</para>
		<screen><![CDATA[
sudo curlftpfs ftp://username:password@172.16.0.1 /mnt/ftp
	]]></screen>
		<para>卸载</para>
		<screen><![CDATA[
sudo fusermount -u /mnt/ftp
	]]></screen>
		<para>权限设置</para>
		<screen><![CDATA[
sudo curlftpfs -o rw,allow_other,uid=500,gid=500 ftp://neo:chen@172.16.1.1 /mnt/ftp
sudo curlftpfs ftp://host/sub_dir mount_point -o user="ftp_username:ftp_password", uid=user_id, gid=group_id, allow_other
	]]></screen>
		<para>fstab 开机自动挂载</para>
		<screen><![CDATA[
sudo echo "curlftpfs#username:password@172.16.0.1 /mnt/ftp fuse allow_other,uid=userid,gid=groupid 0 0" >> /etc/fstab
	]]></screen>
	</section>
	<section id="sshfs">
		<title>SSHFS (sshfs - filesystem client based on SSH File Transfer Protocol)</title>
		<screen><![CDATA[
$ sudo apt-get install sshfs
$ sudo sshfs root@172.16.0.5:/home/neo /mnt
$ sudo fusermount -u /mnt
	    ]]></screen>
	</section>

	<section id="davfs2">
		<title>davfs2 - mount a WebDAV resource as a regular file system</title>
		<para>install</para>
		<screen><![CDATA[
$ sudo apt-get install davfs2
		]]></screen>
		<para>mount a webdav to directory</para>
		<screen><![CDATA[
$ sudo mount.davfs https://opensvn.csie.org/netkiller /mnt/davfs/
Please enter the username to authenticate with server
https://opensvn.csie.org/netkiller or hit enter for none.
Username: svn
Please enter the password to authenticate user svn with server
https://opensvn.csie.org/netkiller or hit enter for none.
Password:
mount.davfs: the server certificate is not trusted
  issuer:      CSIE.org, CSIE.org, Taipei, Taiwan, TW
  subject:     CSIE.org, CSIE.org, Taipei, TW
  identity:    *.csie.org
  fingerprint: e6:05:eb:fb:69:5d:25:4e:11:3c:83:e8:7c:44:ee:bf:a9:85:a3:64
You only should accept this certificate, if you can
verify the fingerprint! The server might be faked
or there might be a man-in-the-middle-attack.
Accept certificate for this session? [y,N] y
		]]></screen>
		<para>test</para>
		<screen><![CDATA[
$ ls davfs/
branches  lost+found  tags  trunk
		]]></screen>
	</section>
	<section id="fs.redisfs">
		<title>redisfs</title>
		<para>Redis Filesystem</para>
		<para></para>
		<screen><![CDATA[
redisfs --host=localhost --port=6379 --mount=/mnt/redis  [--read-only] [--debug] [--prefix=skx]
		]]></screen>
		<para>创建快照</para>
		<screen><![CDATA[
redisfs-snapsot --from=skx --to=snap			
		]]></screen>
		<para>Mount 快照</para>
		<screen><![CDATA[
mkdir /tmp/snapsot
redisfs --prefix=snap --mount=/tmp/snapsot
		]]></screen>
	</section>
	<section id="fs.test">
		<title>File system test</title>
		<para>写写空文件</para>
		<screen><![CDATA[
$ dd bs=1 seek=2TB if=/dev/null of=test
$ time dd if=/dev/zero of=/srv/file bs=1M count=1000
		]]></screen>
		<para>写随机文件</para>
		<screen><![CDATA[
$ time dd if=/dev/urandom of=test.txt bs=1M count=1000		
		]]></screen>
		<section>
			<title>ext4 vs btrfs</title>
			<screen>
			<![CDATA[
$ cat /etc/fstab
# /etc/fstab: static file system information.
#
# Use 'blkid -o value -s UUID' to print the universally unique identifier
# for a device; this may be used with UUID= as a more robust way to name
# devices that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
proc            /proc           proc    nodev,noexec,nosuid 0       0
/dev/sda1       /               ext4    errors=remount-ro 0       1
# /opt was on /dev/sda7 during installation
UUID=5ce518a4-0f46-4688-8002-84bac2330282 /opt            btrfs   defaults        0       2
# /srv was on /dev/sda6 during installation
UUID=19573a64-f0a6-4250-a9fd-532e3d4e3477 /srv            ext4   defaults        0       2
# swap was on /dev/sda5 during installation
UUID=0f2e2f50-d989-47bf-afb7-7593888222cf none            swap    sw              0       0
			]]>
			</screen>
			<screen>
			<![CDATA[
neo@neo-Vostro-3400:~$ time dd if=/dev/zero of=/srv/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.500941 s, 209 MB/s

real	0m0.521s
user	0m0.000s
sys	0m0.140s
neo@neo-Vostro-3400:~$ time dd if=/dev/zero of=/srv/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.672553 s, 156 MB/s

real	0m0.698s
user	0m0.000s
sys	0m0.160s
neo@neo-Vostro-3400:~$ time dd if=/dev/zero of=/opt/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.0987276 s, 1.1 GB/s

real	0m0.133s
user	0m0.000s
sys	0m0.120s
neo@neo-Vostro-3400:~$ time dd if=/dev/zero of=/opt/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.101664 s, 1.0 GB/s

real	0m0.134s
user	0m0.000s
sys	0m0.140s



neo@neo-Vostro-3400:~$ time dd if=/dev/zero of=/srv/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 11.8609 s, 88.4 MB/s

real	0m11.914s
user	0m0.010s
sys	0m1.360s
neo@neo-Vostro-3400:~$ time dd if=/dev/zero of=/opt/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 9.80331 s, 107 MB/s

real	0m9.860s
user	0m0.000s
sys	0m0.880s
neo@neo-Vostro-3400:~$

			]]>
			</screen>
		</section>
		<section>
			<title>xfs vs jfs vs reiserfs</title>
			<screen>
			<![CDATA[
$ cat /etc/fstab
# /etc/fstab: static file system information.
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
proc            /proc           proc    defaults        0       0
# /dev/sda2       /               ext3    errors=remount-ro 0       1
UUID=8ce10c79-f97e-4585-8c07-75f64f043137       /               ext3    errors=remount-ro 0       1
# /dev/sda1       /boot           ext3    defaults        0       2
UUID=35705945-65ed-437b-9f79-fd0e014d100c       /boot           ext3    defaults        0       2
# /dev/sda5       /home           reiserfs defaults        0       2
UUID=f376adb7-e943-4805-892a-4fa457150b66       /home           reiserfs defaults        0       2
# /dev/sda7       /srv            xfs     defaults        0       2
UUID=2ee5c516-707f-47dc-a6a6-0d49d5dc9829       /srv            xfs     defaults        0       2
# /dev/sda8       /var            jfs     defaults        0       2
UUID=2928ba86-72fb-4b60-adc2-0f7d47e62d03       /var            jfs     defaults        0       2
# /dev/sda6       /var/www        ext3    defaults        0       2
UUID=34d3890f-a682-42ea-bdca-815f442e6539       /var/www        ext3    defaults        0       2
# /dev/sda3       none            swap    sw              0       0
UUID=bf605f47-70bc-4653-be98-c8659f959e25       none            swap    sw              0       0
/dev/scd0       /media/cdrom0   udf,iso9660 user,noauto     0       0

			]]>
			</screen>
			<screen>
			<![CDATA[
# XFS

neo@deployment:~$ time dd if=/dev/zero of=/srv/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.0897329 s, 1.2 GB/s

real	0m0.117s
user	0m0.000s
sys	0m0.088s
neo@deployment:~$ time dd if=/dev/zero of=/srv/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 4.86382 s, 216 MB/s

real	0m4.885s
user	0m0.000s
sys	0m0.960s

# JFS

neo@deployment:~$ time dd if=/dev/zero of=/var/tmp/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.127605 s, 822 MB/s

real	0m0.157s
user	0m0.000s
sys	0m0.100s
neo@deployment:~$ time dd if=/dev/zero of=/var/tmp/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 9.58573 s, 109 MB/s

real	0m9.597s
user	0m0.000s
sys	0m0.988s

# reiserfs

neo@deployment:~$ time dd if=/dev/zero of=/home/neo/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.392038 s, 267 MB/s

real	0m0.430s
user	0m0.000s
sys	0m0.252s
neo@deployment:~$ time dd if=/dev/zero of=/home/neo/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 4.62378 s, 227 MB/s

real	0m4.663s
user	0m0.000s
sys	0m2.592s

# EXT3

neo@deployment:~$ time dd if=/dev/zero of=/var/www/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.189314 s, 554 MB/s

real	0m0.207s
user	0m0.004s
sys	0m0.176s
neo@deployment:~$ time dd if=/dev/zero of=/var/www/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 6.46036 s, 162 MB/s

real	0m7.460s
user	0m0.008s
sys	0m1.832s
			]]>
			</screen>
		</section>
		<section>
			<title>RAID10 (146G*8) vs EMC VNX 5300 (8G Fibre Channel)</title>
			<para>服务器RAID卡带宽是6G，而Fibre Channel目前是8G，ISCSI与FCoE 可以提供 10G带宽，InfiniBand可以提供120G带宽。</para>
			<screen><![CDATA[
# cat /etc/fstab
LABEL=/                 /                       ext3    defaults        1 1
LABEL=/boot             /boot                   ext3    defaults        1 2
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
LABEL=SWAP-sda3         swap                    swap    defaults        0 0
/dev/sda4               /home/oracle/rman       ext3    defaults        0 2
/dev/sdb1               /opt/oracle     ext3    defaults        0 2
/dev/emcpowerj1         /u02                    ext3    defaults        0 0

# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda2              95G  7.8G   82G   9% /
/dev/sda1             1.9G   42M  1.8G   3% /boot
tmpfs                  63G  534M   63G   1% /dev/shm
/dev/sda4             924G  619G  258G  71% /home/oracle/rman
/dev/sdb1             1.1T  309G  735G  30% /opt/oracle
/dev/emcpowerj1       296G   20G  261G   7% /u02
			]]></screen>
			<para>IBM X3850 G5</para>
			<screen><![CDATA[
# time dd if=/dev/zero of=file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.152702 seconds, 687 MB/s

real	0m0.154s
user	0m0.001s
sys	0m0.153s
# time dd if=/dev/zero of=file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 1.6589 seconds, 632 MB/s

real	0m1.710s
user	0m0.009s
sys	0m1.657s

# time dd if=/dev/zero of=file bs=1G count=2
2+0 records in
2+0 records out
2147483648 bytes (2.1 GB) copied, 3.48809 seconds, 616 MB/s

real	0m5.899s
user	0m0.000s
sys	0m5.594s
			]]></screen>
			<para>EMC</para>
			<screen><![CDATA[
# time dd if=/dev/zero of=file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.175535 seconds, 597 MB/s

real	0m0.178s
user	0m0.001s
sys	0m0.171s
# time dd if=/dev/zero of=file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 1.67429 seconds, 626 MB/s

real	0m1.718s
user	0m0.002s
sys	0m1.664s
# time dd if=/dev/zero of=file bs=1G count=2
2+0 records in
2+0 records out
2147483648 bytes (2.1 GB) copied, 3.46919 seconds, 619 MB/s

real	0m3.757s
user	0m0.002s
sys	0m3.656s
			]]></screen>
		</section>
		<section>
			<title>Dell 2950(RAID5 500G SATA * 6) vs MD1200 </title>
			<screen>
			<![CDATA[
# cat /etc/fstab
# /etc/fstab: static file system information.
#
# Use 'blkid -o value -s UUID' to print the universally unique identifier
# for a device; this may be used with UUID= as a more robust way to name
# devices that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
proc            /proc           proc    nodev,noexec,nosuid 0       0
# / was on /dev/sda1 during installation
UUID=2fc411ec-9f6e-4e04-9270-11d23a9b0668 /               ext4    errors=remount-ro 0       1
# swap was on /dev/sda2 during installation
UUID=f5175b7a-4c87-471c-ab9f-9d601bc5e6e2 none            swap    sw              0       0
UUID=3217bdd9-1beb-494a-a428-8d1c09eaa1af /backup ext4 errors=remount-ro 0       1
UUID=9bed3b85-bbc5-4aec-8c9a-8911712ea0c6 /backup1 ext4 errors=remount-ro 0       1
UUID=24bec385-2074-4eb5-8d24-22c33dc245d8 /backup2 ext4 errors=remount-ro 0       1


# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1              46G  8.6G   35G  20% /
none                  2.0G  212K  2.0G   1% /dev
none                  2.0G     0  2.0G   0% /dev/shm
none                  2.0G  9.0M  2.0G   1% /var/run
none                  2.0G     0  2.0G   0% /var/lock
none                   46G  8.6G   35G  20% /var/lib/ureadahead/debugfs
/dev/sda3             2.2T  2.0T   89G  96% /backup
/dev/sdc1             7.2T  6.0T  887G  88% /backup2
/dev/sdb1             9.0T  7.0T  1.6T  83% /backup1
			]]>
			</screen>
			<para>/dev/sda 是2950 RAID5 500G*6 1000RPM</para>
			<para>/dev/sdb 是MD1200 RAID5 2T*6 7200RPM</para>
			<para>/dev/sdc 是MD1200 RAID50 2T*6 7200RPM</para>
			<screen><![CDATA[
root@backup:~# time dd if=/dev/zero of=/backup/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.156596 s, 670 MB/s

real	0m0.242s
user	0m0.010s
sys	0m0.150s
root@backup:~# time dd if=/dev/zero of=/backup/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 4.61282 s, 227 MB/s

real	0m4.763s
user	0m0.000s
sys	0m1.640s
root@backup:~# time dd if=/dev/zero of=/backup/file bs=1G count=5
5+0 records in
5+0 records out
5368709120 bytes (5.4 GB) copied, 33.7263 s, 159 MB/s

real	0m34.685s
user	0m0.000s
sys	0m13.070s
root@backup:~# time dd if=/dev/zero of=/backup1/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.130451 s, 804 MB/s

real	0m0.290s
user	0m0.000s
sys	0m0.130s
root@backup:~# time dd if=/dev/zero of=/backup1/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 57.1654 s, 18.3 MB/s

real	0m57.206s
user	0m0.000s
sys	0m1.580s
root@backup:~# time dd if=/dev/zero of=/backup1/file bs=1G count=5
5+0 records in
5+0 records out
5368709120 bytes (5.4 GB) copied, 309.194 s, 17.4 MB/s

real	5m9.762s
user	0m0.000s
sys	0m10.820s
root@backup:~# time dd if=/dev/zero of=/backup2/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 0.145224 s, 722 MB/s

real	0m0.333s
user	0m0.000s
sys	0m0.130s
root@backup:~# time dd if=/dev/zero of=/backup2/file bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 41.9185 s, 25.0 MB/s

real	0m41.979s
user	0m0.010s
sys	0m1.930s
root@backup:~# time dd if=/dev/zero of=/backup2/file bs=1G count=5
5+0 records in
5+0 records out
5368709120 bytes (5.4 GB) copied, 200.384 s, 26.8 MB/s

real	3m20.850s
user	0m0.000s
sys	0m12.490s

			]]></screen>
		</section>
	</section>
	<section id="faq">
		<title>磁盘占用100%删除文件后不是放的解决方法</title>
		<para>首先查看delete状态的进程，然后kill进程，或者重启</para>
		<screen><![CDATA[
lsof | grep delete
		]]></screen>
	</section>
</chapter>
