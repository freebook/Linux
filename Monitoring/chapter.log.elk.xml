<?xml version="1.0" encoding="UTF-8"?>
<chapter id="index"><?dbhtml dir="elk" ?>
	<title>ElasticSearch + Logstash + Kibana</title>
	<chapterinfo>
		<keywordset>
			<keyword>ElasticSearch, Logstash, Kibana</keyword>
		</keywordset>
	</chapterinfo>
	<para>官方网站<ulink url="https://www.elastic.co" /></para>
	<para>环境准备:</para>
	<para>操作系统： CentOS 7</para>
	<para>Java 1.8</para>
	<para>Redis</para>
	<para>ElasticSearch + Logstash + Kibana 均使用 5.2 版本</para>
	<para>以下安装均使用 Netkiller OSCM 脚本一键安装</para>
	<section id="setup">
		<title>ElasticSearch + Logstash + Kibana 安装</title>
		<section>
			<title>ElasticSearch 安装</title>
			<para>粘贴下面命令到Linux控制台即可一键安装</para>
			<screen>
			<![CDATA[
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-5.x.sh | bash
			]]>
			</screen>
		</section>
		<section>
			<title>Kibana 安装</title>
			<screen>
			<![CDATA[
curl -s https://raw.githubusercontent.com/oscm/shell/master/log/kibana/kibana-5.x.sh | bash
			]]>		
			</screen>
		</section>
		<section id="logstash">
			<title>Logstash 安装</title>
			<screen>
curl -s https://raw.githubusercontent.com/oscm/shell/master/log/kibana/logstash-5.x.sh | bash		
			</screen>
		</section>
		<section id="beats">
			<title>Beats 安装</title>
			<screen>
curl -s https://raw.githubusercontent.com/oscm/shell/master/log/beats/beats-5.x.sh | bash
			</screen>
		</section>
	</section>
	<section id="logstash.cli">
		<title>logstash 命令简单应用</title>
		<section>
			<title>-e 命令行运行</title>
			<para>logstash -e "input {stdin{}} output {stdout{}}"</para>
			<screen>
			<![CDATA[
/usr/share/logstash/bin/logstash  -e 'input{file {path => "/etc/centos-release" start_position => "beginning"}} output { stdout {}}'			
			]]>
			</screen>
		</section>
		
		<section>
			<title>-f 指定配置文件</title>
			<para></para>
			<screen>
			<![CDATA[
logstash -f stdin.conf			
			]]>
			</screen>
		</section>
		<section id="configuration">
			<title>-t：测试配置文件是否正确，然后退出。</title>
			<screen>
			<![CDATA[
root@netkiller ~/logstash % /usr/share/logstash/bin/logstash -t -f test.conf
WARNING: Default JAVA_OPTS will be overridden by the JAVA_OPTS defined in the environment. Environment JAVA_OPTS are -server -Xms2048m -Xmx4096m
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console
Configuration OK
			]]>
			</screen>
		</section>
		<section id="output">
			<title>-l：日志输出的地址</title>
			<para>默认就是stdout直接在控制台中输出</para>
		</section>
		<section id="log.level">
			<title>log.level 启动Debug模式</title>
			<screen>
			<![CDATA[
% /usr/share/logstash/bin/logstash -f nginx.conf --path.settings /etc/logstash --log.level debug			
			]]>
			</screen>
		</section>
	</section>
	<section id="redis">
		<title>配置 Broker(Redis)</title>
		<section id="indexer">
			<title>indexer</title>
			<para><graphic  format="png" fileref="../images/elk/Redis.png" srccredit="neo" width=""/></para>
			<para>/etc/logstash/conf.d/indexer.conf</para>
			<screen>
			<![CDATA[
input {
  redis {
    host => "127.0.0.1"
    port => "6379" 
    key => "logstash:demo"
    data_type => "list"
    codec  => "json"
    type => "logstash-redis-demo"
    tags => ["logstashdemo"]
  }
}

output {
  stdout { codec => rubydebug }
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
  }
}	
			]]>
			</screen>
			<para>测试</para>
			<screen>
			<![CDATA[
# redis-cli 
127.0.0.1:6379> RPUSH logstash:demo "{\"time\": \"2012-01-01T10:20:00\", \"message\": \"logstash demo message\"}"
(integer) 1
127.0.0.1:6379> exit
			]]>
			</screen>
			<para>如果执行成功日志如下</para>
			<screen>
			<![CDATA[
# cat /var/log/logstash/logstash-plain.log 
[2017-03-22T15:54:36,491][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://127.0.0.1:9200/]}}
[2017-03-22T15:54:36,496][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://127.0.0.1:9200/, :path=>"/"}
[2017-03-22T15:54:36,600][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>#<URI::HTTP:0x20dae6aa URL:http://127.0.0.1:9200/>}
[2017-03-22T15:54:36,601][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2017-03-22T15:54:36,686][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>50001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"_all"=>{"enabled"=>true, "norms"=>false}, "dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword"}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date", "include_in_all"=>false}, "@version"=>{"type"=>"keyword", "include_in_all"=>false}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2017-03-22T15:54:36,693][INFO ][logstash.outputs.elasticsearch] Installing elasticsearch template to _template/logstash
[2017-03-22T15:54:36,780][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>[#<URI::Generic:0x2f9efc89 URL://127.0.0.1>]}
[2017-03-22T15:54:36,787][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>1000}
[2017-03-22T15:54:36,792][INFO ][logstash.inputs.redis    ] Registering Redis {:identity=>"redis://@127.0.0.1:6379/0 list:logstash:demo"}
[2017-03-22T15:54:36,793][INFO ][logstash.pipeline        ] Pipeline main started
[2017-03-22T15:54:36,838][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2017-03-22T15:55:10,018][WARN ][logstash.runner          ] SIGTERM received. Shutting down the agent.
[2017-03-22T15:55:10,024][WARN ][logstash.agent           ] stopping pipeline {:id=>"main"}			
			]]>
			</screen>
		</section>
		<section id="shipper">
			<title>shipper</title>
			<screen>
			<![CDATA[
input {
  file {
    path => [ "/var/log/nginx/access.log" ]
    start_position => "beginning"
  }
}

filter {
  grok {
    match => { "message" => "%{NGINXACCESS}" }
    add_field => { "type" => "access" }
  }
  date {
    match => [ "timestamp" , "dd/MMM/YYYY:HH:mm:ss Z" ]
  }
  geoip {
    source => "clientip"
  }
}

output {
  redis {
    host => "127.0.0.1"
    port => 6379
    data_type => "list"
    key => "logstash:demo"
  }
}
			]]>
			</screen>
		</section>
	</section>

	<section id="logstash">
		<title>logstash 配置项</title>
		<section id="input">
			<title>input</title>
			<section id="stdin">
				<title>标准输入输出</title>
				<screen>
root@netkiller ~ % /usr/share/logstash/bin/logstash -e "input {stdin{}} output {stdout{}}"
Helloworld
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path //usr/share/logstash/config/log4j2.properties. Using default config which logs to console
18:03:38.340 [[main]-pipeline-manager] INFO  logstash.pipeline - Starting pipeline {"id"=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>1000}
18:03:38.356 [[main]-pipeline-manager] INFO  logstash.pipeline - Pipeline main started
The stdin plugin is now waiting for input:
2017-08-03T10:03:38.375Z localhost Helloworld
18:03:38.384 [Api Webserver] INFO  logstash.agent - Successfully started Logstash API endpoint {:port=>9601}
				</screen>
			</section>
			<section id="rubydebug">
				<title>rubydebug</title>
				<para>rubydebug提供以json格式输出到屏幕</para>
				<screen>
root@netkiller ~ % /usr/share/logstash/bin/logstash -e 'input{stdin{}}output{stdout{codec=>rubydebug}}'
My name is neo
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path //usr/share/logstash/config/log4j2.properties. Using default config which logs to console
18:05:02.734 [[main]-pipeline-manager] INFO  logstash.pipeline - Starting pipeline {"id"=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>1000}
18:05:02.747 [[main]-pipeline-manager] INFO  logstash.pipeline - Pipeline main started
The stdin plugin is now waiting for input:
{
    "@timestamp" => 2017-08-03T10:05:02.764Z,
      "@version" => "1",
          "host" => "localhost",
       "message" => "My name is neo"
}
18:05:02.782 [Api Webserver] INFO  logstash.agent - Successfully started Logstash API endpoint {:port=>9601}
				</screen>
			</section>			
			<section id="file">
				<title>本地文件</title>
				<screen>
				<![CDATA[
input {
  file {
    type => "syslog"
    path => [ "/var/log/maillog", "/var/log/messages", "/var/log/secure" ]
    start_position => "beginning"
  }
}
output {
  stdout { codec => rubydebug }
  elasticsearch { 
    hosts => ["127.0.0.1:9200"] 
  }
}		
				]]>
				</screen>
				<para>start_position => "beginning" 从头开始读，如果没有这个选项，只会读取最后更新的数据。</para>
				<section id="input.type">
					<title>指定文件类型</title>
					<screen>
					<![CDATA[
input { 
 file { path =>"/var/log/messages" type =>"syslog"} 
 file { path =>"/var/log/apache/access.log" type =>"apache"} 
}					 
					]]>
					</screen>
					<section id="type.nginx">
						<title>Nginx</title>
						<screen>
						<![CDATA[
input {
        file {
                type => "nginx_access"
                path => ["/usr/share/nginx/logs/test.access.log"]
        }
}
output {
        redis {
                host => "localhost"
                data_type => "list"
                key => "logstash:redis"
        }
}						
						]]>
						</screen>
					</section>
				</section>
			</section>
			<section id="socket">
				<title>TCP/UDP</title>
				<screen>
				<![CDATA[
input {
  file {
    type => "syslog"
    path => [ "/var/log/secure", "/var/log/messages", "/var/log/syslog" ]
  }
  tcp {
    port => "5145"
    type => "syslog-network"
  }
  udp {
    port => "5145"
    type => "syslog-network"
  }
}
output {
  elasticsearch { 
    hosts => ["127.0.0.1:9200"] 
  }
}
				]]>
				</screen>
			</section>
			<section id="redis">
				<title>Redis</title>
				<screen>
				<![CDATA[
input {
  redis {
    host => "127.0.0.1"
    port => "6379" 
    key => "logstash:demo"
    data_type => "list"
    codec  => "json"
    type => "logstash-redis-demo"
    tags => ["logstashdemo"]
  }
}

output {
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
  }
}
				]]>
				</screen>
				<para>指定 Database 10</para>
				<screen>
				<![CDATA[
root@netkiller /etc/logstash/conf.d % cat spring-boot-redis.conf 
input {
 redis {
  codec => json
  host => "localhost"
  port => 6379
  db => 10
  key => "logstash:redis"
  data_type => "list"
 }
}

output {
  stdout { codec => rubydebug }
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
    index => "logstash-api"
  }
}
				]]>
				</screen>
			</section>
			<section id="kafka">
				<title>Kafka</title>
				<para><graphic  format="png" fileref="../images/elk/Kafka.png" srccredit="neo" width=""/></para>
				<para></para>
				<screen>
				<![CDATA[
input {
  kafka {
   zk_connect => "kafka:2181"
   group_id => "logstash"
   topic_id => "apache_logs"
   consumer_threads => 16
  }
}		
				]]>
				</screen>
			</section>
			<section id="jdbc">
				<title>jdbc</title>
				<screen>
				<![CDATA[
root@netkiller /etc/logstash/conf.d % cat jdbc.conf 
input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "123456"
    schedule => "* * * * *"
    statement => "select * from article where id > :sql_last_value"
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article.last"
  }
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "123456"
    schedule => "* * * * *"	#定时cron的表达式,这里是每分钟执行一次
    statement => "select * from article where ctime > :sql_last_value"
    use_column_value => true
    tracking_column => "ctime"
    tracking_column_type => "timestamp" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article-ctime.last"
  }

}
output {
    elasticsearch {
    	hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"
        action => "update"
        doc_as_upsert => true
    }
}				
				]]>
				</screen>
			</section>
		</section>
		<section id="filter">
			<title>filter</title>
			<section id="filter.patterns">
				<title>patterns</title>
				<para>创建匹配文件 /usr/share/logstash/patterns</para>
				<screen>
				<![CDATA[
mkdir /usr/share/logstash/patterns
vim /usr/share/logstash/patterns

NGUSERNAME [a-zA-Z\.\@\-\+_%]+
NGUSER %{NGUSERNAME}
NGINXACCESS %{IPORHOST:clientip} %{NGUSER:ident} %{NGUSER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response} (?:%{NUMBER:bytes}|-) (?:"(?:%{URI:referrer}|-)"|%{QS:referrer}) %{QS:agent}
				]]>
				</screen>
				<screen>
				<![CDATA[
filter {
  if [type] == "nginx-access" {
    grok {
      match => { "message" => "%{NGINXACCESS}" }
    }
  }
}
				]]>
				</screen>
			</section>
			<section id="syslog">
				<title>syslog</title>
				<screen>
				<![CDATA[
input {
  file {
    type => "syslog" 
    path => [ "/var/log/*.log", "/var/log/messages", "/var/log/syslog" ]
    sincedb_path => "/opt/logstash/sincedb-access"
  }
  syslog {
    type => "syslog"
    port => "5544"
  }
}
 
filter {
  grok {
    type => "syslog"
    match => [ "message", "%{SYSLOGBASE2}" ]
    add_tag => [ "syslog", "grokked" ]
  }
}
 
output {
 elasticsearch { host => "elk.netkiller.cn" }
}				
				]]>
				</screen>
			</section>
			<section id="csv">
				<title>csv</title>
				<screen>
				<![CDATA[
input {
    file {
        type => "SSRCode"
        path => "/SD/2015*/01*/*.csv"
        start_position => "beginning"
    }
}
 
filter {
	csv {
		columns => ["Code","Source"]
		separator => ","
	}
	kv {
		source => "uri"
		field_split => "&?"
		value_split => "="
	}
 
}
 
# output logs to console and to elasticsearch
output {
    stdout {}
    elasticsearch {
        hosts => ["172.16.1.1:9200"]
    }
}				
				]]>
				</screen>
			</section>
			<section id="">
				<title>使用ruby 处理 CSV文件</title>
				<screen>
				<![CDATA[
input {
    stdin {}
}
filter {
    ruby {
        init => "
            begin
                @@csv_file    = 'output.csv'
                @@csv_headers = ['A','B','C']
                if File.zero?(@@csv_file) || !File.exist?(@@csv_file)
                    CSV.open(@@csv_file, 'w') do |csv|
                        csv << @@csv_headers
                    end
                end
            end
        "
        code => "
            begin
                event['@metadata']['csv_file']    = @@csv_file
                event['@metadata']['csv_headers'] = @@csv_headers
            end
        "
    }
    csv {
        columns => ["a", "b", "c"]
    }
}
output {
    csv {
        fields => ["a", "b", "c"]
        path   => "%{[@metadata][csv_file]}"
    }
    stdout {
        codec => rubydebug {
            metadata => true
        }
    }
}				
				]]>
				</screen>
				<para>测试</para>
				<screen>
				<![CDATA[
echo "1,2,3\n4,5,6\n7,8,9" | ./bin/logstash -f csv-headers.conf				
				]]>
				</screen>
				<para>输出结果</para>
				<screen>
				<![CDATA[
A,B,C
1,2,3
4,5,6
7,8,9
				]]>
				</screen>
			</section>
			<section>
				<title>执行 ruby 代码</title>
				<para>日期格式化, 将ISO 8601日期格式转换为 %Y-%m-%d %H:%M:%S</para>
				<para>保存下面内容到配置文件data.conf</para>
				<screen>
				<![CDATA[
input {
	stdin{}
}
filter {

	ruby {
        code => "event.set('ctime', event.get('[ctime]').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }

	ruby {
        code => "event.set('mtime', event.get('[mtime]').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }
}
output {

	stdout {
		codec => rubydebug
	}

}
				]]>
				</screen>
				<para>/usr/share/logstash/bin/logstash -f date.conf</para>
			</section>
			<section id="">
				<title>grok debug 工具</title>
				<para>http://grokdebug.herokuapp.com</para>
			</section>
		</section>
		<section id="output">
			<title>output</title>
			<section id="file">
				<title>file 写入文件</title>
				<screen>
				<![CDATA[
output {
    file {
        path => "/path/to/%{host}/%{+yyyy}/%{+MM}/%{+dd}.log.gz"
        message_format => "%{message}"
        gzip => true
    }
}				
				]]>
				</screen>
			</section>
			<section id="elasticsearch">
				<title>elasticsearch</title>
				
				<screen>
				<![CDATA[
output {
  stdout { codec => rubydebug }
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
    index => "logging"
  }
}				
				]]>
				</screen>
				<section>
					<title>自定义 index</title>
					<para>配置实现每日切割一个 index</para>
					<screen>
					<![CDATA[
index => "logstash-%{+YYYY.MM.dd}"

	
"_index" : "logstash-2017.03.22"	
					]]>
					</screen>
					<para>index 自定义 logstash-%{type}-%{+YYYY.MM.dd}</para>
					<screen>
					<![CDATA[
input {

    redis {
        data_type => "list"
        key => "logstash:redis"
        host => "127.0.0.1"
        port => 6379
        threads => 5
        codec => "json"
    }
}
filter {

}
output {

    elasticsearch {
        hosts => ["127.0.0.1:9200"]
        index => "logstash-%{type}-%{+YYYY.MM.dd}"
        document_type => "%{type}"
        workers => 1
        flush_size => 20
        idle_flush_time => 1
        template_overwrite => true
    }
    stdout{}
}					
					]]>
					</screen>
				</section>
			</section>
			<section>
				<title>exec 执行脚本</title>
				<screen>
				<![CDATA[
output {
    exec {
        command => "sendsms.php \"%{message}\" -t %{user}"
    }
}
				]]>
				</screen>
			</section>
			<section>
				<title>stdout</title>
				<screen>
output {
	stdout { codec => rubydebug }
}
				</screen>
			</section>
		</section>
	</section>
	<section id="logstash.example">
		<title>Example</title>
		<para>https://github.com/kmtong/logback-redis-appender</para>
		<section>
			<title>Spring boot logback</title>
			<example>
				<title>spring boot logback</title>
				<screen>
				<![CDATA[
root@netkiller /etc/logstash/conf.d % cat spring-boot-redis.conf 
input {
 redis {
  codec => json
  host => "localhost"
  port => 6379
  key => "logstash:redis"
  data_type => "list"
 }
}

output {
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
    index => "logstash-api"
  }
}
				
				]]>
				</screen>
				<para>src/main/resources/logback.xml</para>
				<screen>
				<![CDATA[
neo@MacBook-Pro ~/deployment % cat api.netkiller.cn/src/main/resources/logback.xml 
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
	<include resource="org/springframework/boot/logging/logback/defaults.xml" />
	<include resource="org/springframework/boot/logging/logback/file-appender.xml" />
	<property name="type.name" value="test" />
	<appender name="LOGSTASH" class="com.cwbase.logback.RedisAppender">
		<source>mySource</source>
		<sourcePath>mySourcePath</sourcePath>
		<type>myApplication</type>
		<tags>production</tags>
		<host>localhost</host>
		<port>6379</port>
		<database>0</database>
		<key>logstash:api</key>
	</appender>
	<appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
		<encoder>
			<pattern>%date{yyyy-MM-dd HH:mm:ss} %-4relative [%thread] %-5level %logger{35} : %msg %n</pattern>
		</encoder>
	</appender>
	<root level="INFO">
		<appender-ref ref="STDOUT" />
		<appender-ref ref="FILE" />
		<appender-ref ref="LOGSTASH" />
	</root>
</configuration>
				]]>
				</screen>
			</example>
		</section>
		<section>
			<title>索引切割实例</title>
			<example>
				<title>Elasticsearch 索引切割示例</title>
				<screen>
				<![CDATA[
root@netkiller /opt/api.netkiller.cn % cat /etc/logstash/conf.d/spring-boot-redis.conf 
input {
 redis {
  codec => json
  host => "localhost"
  port => 6379
  db => 10
  key => "logstash:redis"
  data_type => "list"
 }
}

output {
  stdout { codec => rubydebug }
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
    index => "logstash-%{type}-%{+YYYY.MM.dd}"
  }
}

				]]>
				</screen>
				<screen>
				<![CDATA[
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
	<include resource="org/springframework/boot/logging/logback/defaults.xml" />
	<include resource="org/springframework/boot/logging/logback/file-appender.xml" />
	<property name="logstash.type" value="api" />
	<property name="logstash.tags" value="springboot" />
	<appender name="LOGSTASH" class="com.cwbase.logback.RedisAppender">
		<source>application.properties</source>
		<type>${logstash.type}</type>
		<tags>${logstash.tags}</tags>

		<host>localhost</host>
		<database>10</database>
		<key>logstash:redis</key>

		<mdc>true</mdc>
		<location>true</location>
		<callerStackIndex>0</callerStackIndex>

	</appender>
	<appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
		<appender-ref ref="LOGSTASH" />
	</appender>

	<appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
		<encoder>
			<pattern>%date{yyyy-MM-dd HH:mm:ss} %-4relative [%thread] %-5level %logger{35} : %msg %n</pattern>
		</encoder>
	</appender>
	<root level="INFO">
		<appender-ref ref="STDOUT" />
		<appender-ref ref="FILE" />
		<appender-ref ref="LOGSTASH" />
	</root>
</configuration>
				
				]]>
				</screen>
			</example>
		</section>
		<section id="">
			<title></title>
			<section>
			<![CDATA[
			
input {
    file {
        path => ["/home/test/data.csv"]
        start_position => "beginning" #从什么位置读取，beginnig时导入原有数据
        sincedb_path => "/test/111"
                type => "csv"
                tags => ["optical", "gather"]
    }
}


filter {
        if [type] == "csv" { #多个配置文件同时执行的区分
        csv {
            columns =>["name","device_id"]
            separator => "^"
			quote_char => "‰"
			remove_field => ["device_id","branch_id","area_type"]
       }
   }
output{
}			
			
			]]>
			</section>
		</section>
	</section>
	<section id="faq">
		<title>FAQ</title>
		<section>
			<title>查看 Kibana 数据库</title>
			<screen>
			<![CDATA[
# curl 'http://localhost:9200/_search?pretty'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : ".kibana",
        "_type" : "config",
        "_id" : "5.2.2",
        "_score" : 1.0,
        "_source" : {
          "buildNum" : 14723
        }
      }
    ]
  }
}			
			]]>
			</screen>
		</section>
		<section>
			<title>logstash 无法写入 elasticsearch</title>
			<para>elasticsearch 的配置不能省略 9200 端口，否则将无法链接elasticsearch</para>
			<screen>
			<![CDATA[
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
  }			
			]]>
			</screen>
		</section>
		<section>
			<title>标准输出</title>
			<screen>
			<![CDATA[
#cd /etc/logstash/conf.d
#vim logstash_server.conf
input {
    redis {
        port => "6379"
        host => "127.0.0.1"
        data_type => "list"
        key => "logstash-redis"
        type => "redis-input"
   }
}
output {
    stdout {
    	codec => rubydebug
    }
}			
			]]>
			</screen>
		</section>
	</section>
</chapter>
